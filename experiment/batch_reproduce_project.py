#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# Set standard output unbuffered
import sys
import io
sys.stdout = io.TextIOWrapper(open(sys.stdout.fileno(), 'wb', 0), write_through=True)
"""
Batch run all records of the project and save results to JSONL file

This script automatically processes all records of the specified project, runs compilation error reproduction verification,
and saves the results of each record to a JSONL file. Supports continuing from where it was interrupted.
The script executes two phases in sequence: the first phase uses the passed successful records for diversification selection,
the second phase re-performs diversification selection based on the first phase results.

Usage:
    python3 batch_reproduce_project.py <project_name> [options]
    
Examples:
    python3 batch_reproduce_project.py llvm
    python3 batch_reproduce_project.py systemd 
    python3 batch_reproduce_project.py llvm --start-from 100 --max-records 50

"""

import argparse
import json
import sys
import time
import subprocess
from pathlib import Path
from datetime import datetime

def load_project_records(project_name):
    """Load all records for the specified project"""
    print(f"ğŸ“š Loading {project_name} project records...")
    
    try:
        # First try to read JSONL file from find_repair_patch directory
        jsonl_file = Path(f"../find_repair_patch/{project_name}_repair_analysis.jsonl")
        
        if jsonl_file.exists():
            print(f"ğŸ“„ Reading from find_repair_patch directory: {jsonl_file}")
            records = []
            
            with open(jsonl_file, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    if not line:
                        continue
                    
                    try:
                        record = json.loads(line)
                        record['_line_number'] = line_num
                        records.append(record)
                    except json.JSONDecodeError as e:
                        print(f"âš ï¸ Line {line_num} JSON format error: {e}")
                        continue
            
            print(f"âœ… ä»JSONLæ–‡ä»¶åŠ è½½äº† {len(records)} æ¡è®°å½•")
            return records
        else:
            print(f"ğŸ“„ JSONLæ–‡ä»¶ä¸å­˜åœ¨: {jsonl_file}")
            print("ğŸ“„ å°è¯•ä»compilation_errorç›®å½•è¯»å–JSONæ•°æ®...")
            
            # ä»compilation_errorç›®å½•è¯»å–JSONæ•°æ®
            json_file = Path(f"../compilation_error/{project_name}_compiler_errors_extracted.json")
            
            if not json_file.exists():
                print(f"âŒ JSONæ–‡ä»¶ä¹Ÿä¸å­˜åœ¨: {json_file}")
                return []
            
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # ä»JSONæ•°æ®ä¸­æå–compiler_errorsæ•°ç»„
            if 'compiler_errors' in data:
                compiler_errors = data['compiler_errors']
                records = []
                for line_num, record in enumerate(compiler_errors, 1):
                    # æ·»åŠ è¡Œå·ä¿¡æ¯
                    record['_line_number'] = line_num
                    # å°†commit_shaæ˜ å°„ä¸ºfailure_commitä»¥ä¿æŒå…¼å®¹æ€§
                    if 'commit_sha' in record:
                        record['failure_commit'] = record['commit_sha']
                    records.append(record)
                
                print(f"âœ… ä»JSONæ–‡ä»¶åŠ è½½äº† {len(records)} æ¡è®°å½•")
                return records
            else:
                print(f"âŒ JSONæ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°compiler_errorså­—æ®µ")
                return []
                
    except Exception as e:
        print(f"âŒ åŠ è½½è®°å½•å¤±è´¥: {e}")
        return []

def select_records_with_diverse_jobs(records, max_records=None, successful_records=None):
    """é€‰æ‹©å…·æœ‰å¤šæ ·åŒ–job_nameçš„è®°å½•ï¼Œç¡®ä¿æ¯æ¬¡å°è¯•ä¸åŒçš„job_nameï¼Œå¹¶ä¸ºæ¯ä¸ªè®°å½•æŒ‡å®šé€‰æ‹©çš„job_name"""
    if not records:
        return []
    
    # ç»Ÿè®¡æ‰€æœ‰å¯ç”¨çš„job_name
    job_name_count = {}
    for record in records:
        job_name = record.get('job_name', '')
        if isinstance(job_name, list):
            for job in job_name:
                job_name_count[job] = job_name_count.get(job, 0) + 1
        else:
            job_name_count[job_name] = job_name_count.get(job_name, 0) + 1
    
    print(f"ğŸ“Š å‘ç° {len(job_name_count)} ç§ä¸åŒçš„job_name:")
    for job_name, count in sorted(job_name_count.items()):
        print(f"   - {job_name}: {count} æ¡è®°å½•")
    
    # æŒ‰job_nameåˆ†ç»„è®°å½•ï¼ŒåŒæ—¶è®°å½•æ¯ä¸ªè®°å½•æ¥è‡ªå“ªä¸ªjob_name
    records_by_job = {}
    record_job_mapping = {}  # è®°å½•æ¯ä¸ªè®°å½•å¯¹åº”çš„job_name
    
    for record in records:
        job_name = record.get('job_name', '')
        if isinstance(job_name, list):
            for job in job_name:
                if job not in records_by_job:
                    records_by_job[job] = []
                records_by_job[job].append(record)
                # ä¸ºæ¯ä¸ªè®°å½•è®°å½•å®ƒå¯¹åº”çš„job_name
                record_id = id(record)
                if record_id not in record_job_mapping:
                    record_job_mapping[record_id] = []
                record_job_mapping[record_id].append(job)
        else:
            if job_name not in records_by_job:
                records_by_job[job_name] = []
            records_by_job[job_name].append(record)
            # ä¸ºæ¯ä¸ªè®°å½•è®°å½•å®ƒå¯¹åº”çš„job_name
            record_id = id(record)
            record_job_mapping[record_id] = [job_name]
    
    # ç¬¬ä¸€é˜¶æ®µï¼šè½®è¯¢é€‰æ‹©è®°å½•ï¼Œç¡®ä¿æ¯ä¸ªjob_nameéƒ½æœ‰æœºä¼šè¢«é€‰æ‹©
    selected_records = []
    job_names = list(records_by_job.keys())
    job_indices = {job: 0 for job in job_names}
    
    # æ¯ä¸ªjob_nameé€‰æ‹©1æ¡è®°å½•
    records_per_job = 1
    
    print(f"ğŸ”„ ç¬¬ä¸€é˜¶æ®µï¼šæ¯ä¸ªjob_nameé€‰æ‹© {records_per_job} æ¡è®°å½•...")
    
    # è½®è¯¢æ¯ä¸ªjob_nameï¼Œæ¯ä¸ªé€‰æ‹©æŒ‡å®šæ•°é‡çš„è®°å½•
    for job_name in job_names:
        job_records = records_by_job[job_name]
        for i in range(min(records_per_job, len(job_records))):
            if job_indices[job_name] < len(job_records):
                record = job_records[job_indices[job_name]]
                
                # ä¸ºè®°å½•æ·»åŠ selected_job_nameå­—æ®µ
                record_copy = record.copy()
                record_copy['selected_job_name'] = job_name
                
                selected_records.append(record_copy)
                job_indices[job_name] += 1
    
    print(f"âœ… ç¬¬ä¸€é˜¶æ®µé€‰æ‹©äº† {len(selected_records)} æ¡è®°å½•")
    
    # é™åˆ¶è®°å½•æ•°é‡
    if max_records and len(selected_records) > max_records:
        selected_records = selected_records[:max_records]
        print(f"ğŸ“Œ é™åˆ¶ä¸ºå‰{max_records}æ¡è®°å½•")
    
    print(f"âœ… æ€»å…±é€‰æ‹©äº† {len(selected_records)} æ¡è®°å½•ï¼Œæ¶µç›– {len(set(job_names))} ç§ä¸åŒçš„job_name")
    return selected_records

def select_records_with_successful_jobs(records, max_records=None, successful_records=None):
    """ç¬¬äºŒé˜¶æ®µï¼šåªé€‰æ‹©åŒ…å«æˆåŠŸè¿‡çš„job nameçš„è®°å½•"""
    if not records:
        return []
    
    # ä»æˆåŠŸè®°å½•ä¸­æå–æˆåŠŸè¿‡çš„job name
    successful_job_names = set()
    if successful_records:
        for record_id, record in successful_records.items():
            job_name = record.get('selected_job_name', '')
            if not job_name:
                job_name = record.get('job_name', '')
            if isinstance(job_name, list):
                job_name = job_name[0] if job_name else ''
            if job_name:
                successful_job_names.add(job_name)
    
    print(f"ğŸ“Š å‘ç° {len(successful_job_names)} ä¸ªæˆåŠŸè¿‡çš„job name: {list(successful_job_names)}")
    
    if not successful_job_names:
        print("âš ï¸ æ²¡æœ‰å‘ç°æˆåŠŸè¿‡çš„job nameï¼Œè¿”å›ç©ºåˆ—è¡¨")
        return []
    
    # åªé€‰æ‹©åŒ…å«æˆåŠŸjob nameçš„è®°å½•
    selected_records = []
    
    for record in records:
        record_job_names = record.get('job_name', [])
        if isinstance(record_job_names, str):
            record_job_names = [record_job_names]
        
        # æ£€æŸ¥è¿™ä¸ªè®°å½•æ˜¯å¦åŒ…å«æˆåŠŸè¿‡çš„job name
        has_successful_job = any(job in successful_job_names for job in record_job_names)
        
        if has_successful_job:
            # ä¸ºè®°å½•æ·»åŠ selected_job_nameå­—æ®µï¼Œä¼˜å…ˆé€‰æ‹©æˆåŠŸè¿‡çš„job name
            record_copy = record.copy()
            
            # æ‰¾åˆ°ç¬¬ä¸€ä¸ªæˆåŠŸè¿‡çš„job name
            selected_job = None
            for job in record_job_names:
                if job in successful_job_names:
                    selected_job = job
                    break
            
            # å¦‚æœæ²¡æœ‰æˆåŠŸè¿‡çš„job nameï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ª
            if not selected_job and record_job_names:
                selected_job = record_job_names[0]
            
            record_copy['selected_job_name'] = selected_job or ''
            selected_records.append(record_copy)
    
    print(f"ğŸ“Š æ‰¾åˆ° {len(selected_records)} æ¡åŒ…å«æˆåŠŸjob nameçš„è®°å½•")
    
    # é™åˆ¶è®°å½•æ•°é‡
    if max_records and len(selected_records) > max_records:
        selected_records = selected_records[:max_records]
        print(f"ğŸ“Œ é™åˆ¶ä¸ºå‰{max_records}æ¡è®°å½•")
    
    print(f"âœ… ç¬¬äºŒé˜¶æ®µé€‰æ‹©äº† {len(selected_records)} æ¡è®°å½•ï¼ˆä»…åŒ…å«æˆåŠŸè¿‡çš„job nameï¼‰")
    return selected_records

def load_completed_records(output_file):
    """åŠ è½½å·²å®Œæˆçš„è®°å½•ï¼Œè¿”å›å·²å¤„ç†çš„è®°å½•é›†åˆå’ŒæˆåŠŸè®°å½•å­—å…¸"""
    completed_records = set()
    successful_records = {}  # è®°å½•æˆåŠŸçš„è®°å½•ï¼Œç”¨äºupdateæ¨¡å¼
    
    if not Path(output_file).exists():
        print(f"ğŸ“„ è¾“å‡ºæ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†ä»å¤´å¼€å§‹å¤„ç†")
        return completed_records, successful_records
    
    print(f"ğŸ“„ æ£€æŸ¥å·²å®Œæˆçš„è®°å½•...")
    
    try:
        with open(output_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                
                try:
                    record = json.loads(line)
                    # ä½¿ç”¨failure_commit + job_name + workflow_nameä½œä¸ºå”¯ä¸€æ ‡è¯†
                    # ä¼˜å…ˆä½¿ç”¨selected_job_nameï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨job_name
                    job_name = record.get('selected_job_name', '')
                    if not job_name:
                        job_name = record.get('job_name', '')
                    
                    # ç¡®ä¿job_nameå’Œworkflow_nameæ˜¯å­—ç¬¦ä¸²ï¼Œå¦‚æœæ˜¯åˆ—è¡¨åˆ™å–ç¬¬ä¸€ä¸ªå…ƒç´ 
                    if isinstance(job_name, list):
                        job_name = job_name[0] if job_name else ''
                    
                    workflow_name = record.get('workflow_name', '')
                    if isinstance(workflow_name, list):
                        workflow_name = workflow_name[0] if workflow_name else ''
                    
                    record_id = (
                        record.get('failure_commit', ''),
                        job_name,
                        workflow_name
                    )
                    completed_records.add(record_id)
                    
                    # å¦‚æœæ˜¯æˆåŠŸçš„è®°å½•ï¼Œä¿å­˜åˆ°æˆåŠŸè®°å½•å­—å…¸ä¸­
                    if record.get('reproduce', False):
                        successful_records[record_id] = record
                        
                except json.JSONDecodeError:
                    continue
        
        print(f"âœ… å‘ç° {len(completed_records)} æ¡å·²å®Œæˆçš„è®°å½•")
        print(f"âœ… å…¶ä¸­ {len(successful_records)} æ¡æ˜¯æˆåŠŸçš„è®°å½•")
        return completed_records, successful_records
        
    except Exception as e:
        print(f"âš ï¸ è¯»å–å·²å®Œæˆè®°å½•æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return completed_records, successful_records





def clean_bitcoin_docker():
    """æ¸…ç†Bitcoiné¡¹ç›®çš„Dockerå®¹å™¨"""
    try:
        # å°è¯•åˆ é™¤ci_win64å®¹å™¨
        result = subprocess.run(
            ["docker", "rm", "ci_win64","-f"],
            capture_output=True,
            text=True
        )
        
        if result.returncode == 0:
            print("ğŸ³ å·²æ¸…ç†Dockerå®¹å™¨ ci_win64")
        else:
            # å®¹å™¨å¯èƒ½ä¸å­˜åœ¨ï¼Œè¿™æ˜¯æ­£å¸¸çš„
            if "No such container" not in result.stderr:
                print(f"âš ï¸ æ¸…ç†Dockerå®¹å™¨æ—¶å‡ºç°è­¦å‘Š: {result.stderr.strip()}")
        
    except Exception as e:
        print(f"âš ï¸ æ¸…ç†Dockerå®¹å™¨æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")





def run_single_reproduce(record, project_name, timeout=1800):
    """è¿è¡Œå•æ¡è®°å½•çš„å¤ç°æµ‹è¯•ï¼Œè¿”å›ç»“æœè®°å½•"""
    line_num = record.get('_line_number', '?')
    failure_commit = record.get('failure_commit', '')
    workflow_name = record.get('workflow_name', '')
    
    # ä¼˜å…ˆä½¿ç”¨selected_job_nameï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨job_name
    job_name = record.get('selected_job_name', '')
    if not job_name:
        job_name = record.get('job_name', '')
    
    workflow_id = record.get('workflow_id', '0')  # è·å–workflow_idï¼Œé»˜è®¤ä¸º'0'
    
    # ç¡®ä¿job_nameå’Œworkflow_nameæ˜¯å­—ç¬¦ä¸²ï¼Œå¦‚æœæ˜¯åˆ—è¡¨åˆ™å–ç¬¬ä¸€ä¸ªå…ƒç´ 
    if isinstance(job_name, list):
        job_name = job_name[0] if job_name else ''
    
    if isinstance(workflow_name, list):
        workflow_name = workflow_name[0] if workflow_name else ''
    
    # ç¡®ä¿workflow_idæ˜¯å­—ç¬¦ä¸²
    if isinstance(workflow_id, int):
        workflow_id = str(workflow_id)
    
    # é’ˆå¯¹bitcoiné¡¹ç›®ï¼Œæ¯æ¬¡è¿è¡Œå‰æ¸…ç†Dockerå®¹å™¨
    if project_name.lower() == 'bitcoin':
        clean_bitcoin_docker()
    
    # åˆ›å»ºæ—¥å¿—ç›®å½•
    log_dir = Path("logs")
    log_dir.mkdir(exist_ok=True)
    
    print(f"\n{'='*80}")
    print(f"ğŸ”„ å¤„ç†è®°å½• {line_num}")
    print(f"   Commit: {failure_commit}")
    print(f"   Job: {job_name}")
    print(f"   Workflow: {workflow_name}")
    print(f"   Workflow ID: {workflow_id}")
    print(f"   ğŸ“ æ—¥å¿—æ–‡ä»¶: logs/act_{project_name}_line{line_num}.log")
    print(f"{'='*80}")
    
    start_time = time.time()
    
    # æ„å»ºå‘½ä»¤
    cmd = [
        'python3', 'reproduce/reproduce_compiler_errors.py',
        project_name, failure_commit, job_name, workflow_name, workflow_id,
        '--line-number', str(line_num),
        '--output', f'temp_result_{line_num}.json'
    ]


    try:
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True
        )
        
        success = result.returncode == 0
        elapsed_time = time.time() - start_time
        
        # å°è¯•è¯»å–è¯¦ç»†ç»“æœ
        detailed_result = None
        temp_result_file = f'temp_result_{line_num}.json'
        if Path(temp_result_file).exists():
            try:
                with open(temp_result_file, 'r', encoding='utf-8') as f:
                    detailed_result = json.load(f)
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                Path(temp_result_file).unlink()
            except Exception as e:
                print(f"âš ï¸ è¯»å–è¯¦ç»†ç»“æœå¤±è´¥: {e}")
        
        # æ„å»ºç»“æœè®°å½• - å¤åˆ¶åŸå§‹è®°å½•çš„æ‰€æœ‰å­—æ®µ
        result_record = record.copy()  # å¤åˆ¶åŸå§‹è®°å½•çš„æ‰€æœ‰å­—æ®µ
        
        # ç§»é™¤ä¸´æ—¶æ·»åŠ çš„è¡Œå·å­—æ®µ
        if '_line_number' in result_record:
            del result_record['_line_number']
        
        # ç¡®ä¿job_nameæ˜¯æˆåŠŸçš„é‚£ä¸ªjob name
        selected_job_name = record.get('selected_job_name', '')
        if selected_job_name:
            result_record['job_name'] = selected_job_name
        
        # æ·»åŠ å¤ç°ç»“æœå­—æ®µ
        result_record['reproduce'] = success
        
        # æ·»åŠ å¤ç°ç›¸å…³çš„å…ƒæ•°æ®ï¼ˆå¯é€‰ï¼‰
        result_record['reproduce_metadata'] = {
            'elapsed_time': elapsed_time,
            'timestamp': datetime.now().isoformat(),
            'detailed_result': detailed_result
        }
        
        # å¦‚æœå¤ç°å¤±è´¥ï¼Œæ·»åŠ é”™è¯¯ä¿¡æ¯
        if not success:
            result_record['reproduce_metadata']['error_stdout'] = result.stdout[-1000:] if result.stdout else ""
            result_record['reproduce_metadata']['error_stderr'] = result.stderr[-500:] if result.stderr else ""
        
        if success:
            print(f"âœ… è®°å½• {line_num} å¤„ç†æˆåŠŸ (è€—æ—¶: {elapsed_time:.1f}ç§’)")
            # æ‰“å°è¯¦ç»†çš„æˆåŠŸä¿¡æ¯ï¼Œæ–¹ä¾¿äººå·¥æŸ¥éªŒ
            if detailed_result:
                print(f"   æˆåŠŸé˜¶æ®µ: {detailed_result.get('stage', 'unknown')}")
                print(f"   æˆåŠŸåŸå› : {detailed_result.get('reason', 'unknown')}")
                actual_errors = detailed_result.get('actual_errors', [])
                if actual_errors:
                    print("   å®é™…é”™è¯¯:")
                    for err in actual_errors[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                        print(f"      - {err}")
                    if len(actual_errors) > 3:
                        print(f"      ... (è¿˜æœ‰ {len(actual_errors) - 3} ä¸ªé”™è¯¯)")
                expected_errors = detailed_result.get('expected_errors', [])
                if expected_errors:
                    print("   é¢„æœŸé”™è¯¯:")
                    for err in expected_errors[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                        if isinstance(err, dict) and 'error_lines' in err:
                            # æ–°æ ¼å¼: {'error_lines': [err]}
                            error_text = err['error_lines'][0] if err['error_lines'] else str(err)
                        elif isinstance(err, dict) and 'error_text' in err:
                            # æ—§æ ¼å¼: {'error_text': err}
                            error_text = err.get('error_text', '')
                        else:
                            # å…¶ä»–æ ¼å¼
                            error_text = str(err)
                        print(f"      - {error_text}")
                    if len(expected_errors) > 3:
                        print(f"      ... (è¿˜æœ‰ {len(expected_errors) - 3} ä¸ªé”™è¯¯)")
                if 'comparison' in detailed_result and detailed_result['comparison']:
                    comparison = detailed_result['comparison']
                    print(f"   åŒ¹é…ç»Ÿè®¡: {comparison.get('match_count', 0)}/{comparison.get('expected_count', 0)} åŒ¹é…æˆåŠŸ")
                    print(f"   ç›¸ä¼¼åº¦: {comparison.get('similarity_score', 0):.3f}")
        else:
            print(f"âŒ è®°å½• {line_num} å¤„ç†å¤±è´¥ (è€—æ—¶: {elapsed_time:.1f}ç§’)")
            # æ‰“å°è¯¦ç»†çš„å¤±è´¥åŸå› 
            if detailed_result:
                print(f"   å¤±è´¥é˜¶æ®µ: {detailed_result.get('stage', 'unknown')}")
                print(f"   å¤±è´¥åŸå› : {detailed_result.get('reason', 'unknown')}")
                actual_errors = detailed_result.get('actual_errors', [])
                if actual_errors:
                    print("   å®é™…é”™è¯¯:")
                    for err in actual_errors: 
                        print(f"      - {err}")
                expected_errors = detailed_result.get('expected_errors', [])
                if expected_errors:
                    print("   é¢„æœŸé”™è¯¯:")
                    for err in expected_errors: 
                        if isinstance(err, dict) and 'error_lines' in err:
                            # æ–°æ ¼å¼: {'error_lines': [err]}
                            error_text = err['error_lines'][0] if err['error_lines'] else str(err)
                        elif isinstance(err, dict) and 'error_text' in err:
                            # æ—§æ ¼å¼: {'error_text': err}
                            error_text = err.get('error_text', '')
                        else:
                            # å…¶ä»–æ ¼å¼
                            error_text = str(err)
                        print(f"      - {error_text}")
            if result.stderr:
                print(f"   é”™è¯¯è¾“å‡º: {result.stderr[-200:]}")
        
        return result_record
        
    except subprocess.TimeoutExpired:
        elapsed_time = time.time() - start_time
        print(f"â° è®°å½• {line_num} å¤„ç†è¶…æ—¶ (è€—æ—¶: {elapsed_time:.1f}ç§’)")
        print(f"   è¶…æ—¶é™åˆ¶: {timeout}ç§’")
        
        # è®°å½•è¶…æ—¶ç»“æœ - å¤åˆ¶åŸå§‹è®°å½•çš„æ‰€æœ‰å­—æ®µ
        result_record = record.copy()  # å¤åˆ¶åŸå§‹è®°å½•çš„æ‰€æœ‰å­—æ®µ
        
        # ç§»é™¤ä¸´æ—¶æ·»åŠ çš„è¡Œå·å­—æ®µ
        if '_line_number' in result_record:
            del result_record['_line_number']
        
        # ç¡®ä¿job_nameæ˜¯æˆåŠŸçš„é‚£ä¸ªjob name
        selected_job_name = record.get('selected_job_name', '')
        if selected_job_name:
            result_record['job_name'] = selected_job_name
        
        # æ·»åŠ å¤ç°ç»“æœå­—æ®µ
        result_record['reproduce'] = False
        
        # æ·»åŠ å¤ç°ç›¸å…³çš„å…ƒæ•°æ®
        result_record['reproduce_metadata'] = {
            'elapsed_time': elapsed_time,
            'timestamp': datetime.now().isoformat(),
            'error': 'timeout',
            'error_stderr': "å¤„ç†è¶…æ—¶"
        }
        
        return result_record
        
    except Exception as e:
        elapsed_time = time.time() - start_time
        print(f"âŒ è®°å½• {line_num} å¤„ç†å¼‚å¸¸ (è€—æ—¶: {elapsed_time:.1f}ç§’)")
        print(f"   å¼‚å¸¸ç±»å‹: {type(e).__name__}")
        print(f"   å¼‚å¸¸ä¿¡æ¯: {str(e)}")
        
        # è®°å½•å¼‚å¸¸ç»“æœ - å¤åˆ¶åŸå§‹è®°å½•çš„æ‰€æœ‰å­—æ®µ
        result_record = record.copy()  # å¤åˆ¶åŸå§‹è®°å½•çš„æ‰€æœ‰å­—æ®µ
        
        # ç§»é™¤ä¸´æ—¶æ·»åŠ çš„è¡Œå·å­—æ®µ
        if '_line_number' in result_record:
            del result_record['_line_number']
        
        # ç¡®ä¿job_nameæ˜¯æˆåŠŸçš„é‚£ä¸ªjob name
        selected_job_name = record.get('selected_job_name', '')
        if selected_job_name:
            result_record['job_name'] = selected_job_name
        
        # æ·»åŠ å¤ç°ç»“æœå­—æ®µ
        result_record['reproduce'] = False
        
        # æ·»åŠ å¤ç°ç›¸å…³çš„å…ƒæ•°æ®
        result_record['reproduce_metadata'] = {
            'elapsed_time': elapsed_time,
            'timestamp': datetime.now().isoformat(),
            'error': str(e),
            'error_stderr': str(e)
        }
        
        return result_record

def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(
        description="Batch run all records of the project and save results to JSONL file",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    parser.add_argument('project_name', help='é¡¹ç›®åç§° (å¦‚: llvm, systemd, opencv)')
    parser.add_argument('--start-from', type=int, default=1, help='ä»æŒ‡å®šè¡Œå·å¼€å§‹å¤„ç† (é»˜è®¤: 1)')
    parser.add_argument('--max-records', type=int, help='æœ€å¤§å¤„ç†è®°å½•æ•°')
    parser.add_argument('--timeout', type=int, default=18000, help='å•ä¸ªè®°å½•çš„è¶…æ—¶æ—¶é—´(ç§’) (é»˜è®¤: 3600)')
    
    args = parser.parse_args()
    
    print(f"ğŸš€ å¼€å§‹æ‰¹é‡å¤„ç†{args.project_name}é¡¹ç›®çš„ç¼–è¯‘é”™è¯¯å¤ç°æµ‹è¯•")
    print("=" * 80)
    
    # åŠ è½½è®°å½•
    records = load_project_records(args.project_name)
    if not records:
        print("âŒ æœªæ‰¾åˆ°å¯å¤„ç†çš„è®°å½•")
        sys.exit(1)
    
    if args.max_records is None:
        args.max_records = len(records)
    
    # æ ¹æ®å‚æ•°è¿‡æ»¤è®°å½•
    if args.start_from > 1:
        records = [r for r in records if r.get('_line_number', 0) >= args.start_from]
        print(f"ğŸ“Œ ä»ç¬¬{args.start_from}è¡Œå¼€å§‹å¤„ç†ï¼Œå‰©ä½™ {len(records)} æ¡è®°å½•")
    
    # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶åï¼ˆå›ºå®šåç§°ï¼Œæ— æ—¶é—´æˆ³ï¼‰
    output_dir = Path("reproduce/output")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    output_file = output_dir / f"{args.project_name}_batch_reproduce_results.jsonl"
    backup_file = output_dir / f"{args.project_name}_batch_reproduce_results_backup.jsonl"
    
    # åˆ é™¤æ—§æ–‡ä»¶ï¼Œæ¯æ¬¡éƒ½ä»å¤´å¼€å§‹
    if output_file.exists():
        print(f"ğŸ—‘ï¸ åˆ é™¤æ—§çš„ç»“æœæ–‡ä»¶: {output_file}")
        output_file.unlink()
    
    if backup_file.exists():
        print(f"ğŸ—‘ï¸ åˆ é™¤æ—§çš„å¤‡ä»½æ–‡ä»¶: {backup_file}")
        backup_file.unlink()
    
    # ==================== ç¬¬ä¸€é˜¶æ®µ ====================
    print(f"\nğŸ¯ å¼€å§‹ç¬¬ä¸€é˜¶æ®µå¤„ç†...")
    print(f"=" * 80)
    
    # æ¯æ¬¡éƒ½ä»å¤´å¼€å§‹ï¼Œä¸ä½¿ç”¨å·²å®Œæˆçš„è®°å½•
    completed_records = set()
    successful_records = {}
    
    # ç¬¬ä¸€é˜¶æ®µï¼šä½¿ç”¨å¤šæ ·åŒ–é€‰æ‹©ç­–ç•¥
    print(f"ğŸ”„ ç¬¬ä¸€é˜¶æ®µï¼šä½¿ç”¨å¤šæ ·åŒ–é€‰æ‹©ç­–ç•¥...")
    stage1_records = select_records_with_diverse_jobs(records, args.max_records, successful_records)
    
    if args.max_records and len(stage1_records) > args.max_records:
        stage1_records = stage1_records[:args.max_records]
        print(f"ğŸ“Œ é™åˆ¶å¤„ç†å‰{args.max_records}æ¡è®°å½•")
    
    print(f"ğŸ“„ ç»“æœå°†ä¿å­˜åˆ°: {output_file}")
    print(f"ğŸ“Š ç¬¬ä¸€é˜¶æ®µæ€»è®¡éœ€è¦å¤„ç† {len(stage1_records)} æ¡è®°å½•")
    
    # ç¨‹åºå¼€å§‹å‰åˆ é™¤å¤‡ä»½æ–‡ä»¶
    if backup_file.exists():
        print(f"ğŸ—‘ï¸ åˆ é™¤æ—§çš„å¤‡ä»½æ–‡ä»¶: {backup_file}")
        backup_file.unlink()
    
    # å¤„ç†ç¬¬ä¸€é˜¶æ®µæ‰€æœ‰è®°å½•
    all_records_to_process = stage1_records
    
    if not all_records_to_process:
        print(f"âœ… ç¬¬ä¸€é˜¶æ®µæ²¡æœ‰éœ€è¦å¤„ç†çš„è®°å½•ï¼")
        print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"   æ€»è®°å½•æ•°: {len(records)}")
        print(f"   å¾…å¤„ç†è®°å½•æ•°: 0")
    else:
        print(f"ğŸ“‹ ç¬¬ä¸€é˜¶æ®µéœ€è¦å¤„ç† {len(all_records_to_process)} æ¡è®°å½•")
        
        # æ‰¹é‡å¤„ç†ç¬¬ä¸€é˜¶æ®µ
        success_count = 0
        total_start_time = time.time()
        results = []  # å­˜å‚¨æ‰€æœ‰ç»“æœè®°å½•
        
        for i, record in enumerate(all_records_to_process, 1):
            print(f"\nğŸ“ˆ ç¬¬ä¸€é˜¶æ®µè¿›åº¦: {i}/{len(all_records_to_process)} ({i/len(all_records_to_process)*100:.1f}%)")
            
            # æ¯æ¬¡éƒ½è¿è¡Œå¤ç°æµ‹è¯•ï¼Œä¸æ£€æŸ¥æ˜¯å¦å·²æˆåŠŸ
            result_record = run_single_reproduce(record, args.project_name, args.timeout)
            results.append(result_record)
            
            if result_record.get('reproduce', False):
                success_count += 1
            
            # æ¯æ¬¡å¤„ç†å®Œä¸€æ¡è®°å½•åï¼Œç«‹å³å†™å…¥å¤‡ä»½æ–‡ä»¶ï¼ˆçƒ­æ›´æ–°ï¼‰
            print(f"ğŸ“ æ­£åœ¨ä¿å­˜ç¬¬ {i} æ¡è®°å½•åˆ°å¤‡ä»½æ–‡ä»¶...")
            with open(backup_file, 'w', encoding='utf-8') as f:
                for result_record in results:
                    f.write(json.dumps(result_record, ensure_ascii=False) + '\n')
            
            # æ¯å¤„ç†10æ¡è®°å½•æ˜¾ç¤ºä¸€æ¬¡ç»Ÿè®¡
            if i % 10 == 0 or i == len(all_records_to_process):
                elapsed = time.time() - total_start_time
                avg_time = elapsed / i
                remaining = (len(all_records_to_process) - i) * avg_time
                
                print(f"\nğŸ“Š ç¬¬ä¸€é˜¶æ®µé˜¶æ®µæ€§ç»Ÿè®¡ ({i}/{len(all_records_to_process)}):")
                print(f"   æˆåŠŸ: {success_count}/{i} ({success_count/i*100:.1f}%)")
                print(f"   å¹³å‡è€—æ—¶: {avg_time:.1f}ç§’/æ¡")
                print(f"   å·²ç”¨æ—¶é—´: {elapsed/60:.1f}åˆ†é’Ÿ")
                print(f"   é¢„è®¡å‰©ä½™: {remaining/60:.1f}åˆ†é’Ÿ")
        
        # å¤„ç†å®Œæˆåï¼Œå°†å¤‡ä»½æ–‡ä»¶æ›¿æ¢ä¸ºæœ€ç»ˆæ–‡ä»¶å¹¶åˆ é™¤å¤‡ä»½æ–‡ä»¶
        print(f"ğŸ“ æ­£åœ¨å°†å¤‡ä»½æ–‡ä»¶æ›¿æ¢ä¸ºæœ€ç»ˆæ–‡ä»¶...")
        import os
        os.replace(backup_file, output_file)
        print(f"âœ… ç¬¬ä¸€é˜¶æ®µæ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
        # ç¬¬ä¸€é˜¶æ®µæœ€ç»ˆç»Ÿè®¡
        total_elapsed = time.time() - total_start_time
        
        # è®¡ç®—æ€»ä½“æˆåŠŸæ•°é‡ï¼ˆåŒ…æ‹¬ä¹‹å‰æˆåŠŸçš„è®°å½•ï¼‰
        total_success_count = success_count  # ç°åœ¨success_countå·²ç»åŒ…å«äº†æ‰€æœ‰æˆåŠŸçš„è®°å½•
        
        print(f"\nğŸ¯ ç¬¬ä¸€é˜¶æ®µæ‰¹é‡å¤„ç†å®Œæˆï¼")
        print(f"=" * 80)
        print(f"ğŸ“Š ç¬¬ä¸€é˜¶æ®µæœ€ç»ˆç»Ÿè®¡:")
        print(f"   æ€»è®°å½•æ•°: {len(records)}")
        print(f"   æœ¬æ¬¡å¤„ç†è®°å½•æ•°: {len(all_records_to_process)}")
        print(f"   æœ¬æ¬¡æˆåŠŸæ•°é‡: {success_count}")
        print(f"   æœ¬æ¬¡å¤±è´¥æ•°é‡: {len(all_records_to_process) - success_count}")
        print(f"   æ€»æˆåŠŸæ•°é‡: {total_success_count}")
        if len(all_records_to_process) > 0:
            print(f"   æœ¬æ¬¡æˆåŠŸç‡: {success_count/len(all_records_to_process)*100:.1f}%")
        print(f"   æ€»ä½“æˆåŠŸç‡: {total_success_count/len(records)*100:.1f}%")
        print(f"   æ€»è€—æ—¶: {total_elapsed/60:.1f}åˆ†é’Ÿ")
        if len(all_records_to_process) > 0:
            print(f"   å¹³å‡è€—æ—¶: {total_elapsed/len(all_records_to_process):.1f}ç§’/æ¡")
        print(f"ğŸ“„ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
        # ç”Ÿæˆç¬¬ä¸€é˜¶æ®µç®€è¦ç»Ÿè®¡æ–‡ä»¶
        summary_file = output_dir / f"{args.project_name}_batch_reproduce_summary.json"
        summary = {
            'total_records': len(records),
            'processed_records': len(all_records_to_process),
            'success_count': success_count,
            'failure_count': len(all_records_to_process) - success_count,
            'total_success_count': total_success_count,
            'success_rate': success_count / len(all_records_to_process) if len(all_records_to_process) > 0 else 0,
            'overall_success_rate': total_success_count / len(records) if len(records) > 0 else 0,
            'total_elapsed_seconds': total_elapsed,
            'average_time_per_record': total_elapsed / len(all_records_to_process) if len(all_records_to_process) > 0 else 0,
            'timestamp': datetime.now().isoformat(),
            'output_file': str(output_file),
            'stage': 1
        }
        
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“ˆ ç¬¬ä¸€é˜¶æ®µç»Ÿè®¡æ‘˜è¦å·²ä¿å­˜åˆ°: {summary_file}")
    
    # ==================== ç¬¬äºŒé˜¶æ®µ ====================
    print(f"\nğŸ¯ å¼€å§‹ç¬¬äºŒé˜¶æ®µå¤„ç†...")
    print(f"=" * 80)
    
    # ç¬¬äºŒé˜¶æ®µï¼šé‡æ–°åŠ è½½ç¬¬ä¸€é˜¶æ®µçš„è¿è¡Œç»“æœï¼Œåªé€‰æ‹©æˆåŠŸè¿‡çš„job name
    print(f"ğŸ”„ ç¬¬äºŒé˜¶æ®µï¼šé‡æ–°åŠ è½½ç¬¬ä¸€é˜¶æ®µçš„è¿è¡Œç»“æœï¼Œåªé€‰æ‹©æˆåŠŸè¿‡çš„job name...")
    
    # é‡æ–°åŠ è½½ç¬¬ä¸€é˜¶æ®µçš„è¿è¡Œç»“æœ
    updated_completed_records, updated_successful_records = load_completed_records(output_file)
    print(f"ğŸ“Š ç¬¬ä¸€é˜¶æ®µè¿è¡Œåï¼ŒæˆåŠŸè®°å½•æ•°: {len(updated_successful_records)}")
    
    # ä½¿ç”¨æ›´æ–°åçš„successful_recordsï¼Œåªé€‰æ‹©æˆåŠŸè¿‡çš„job name
    stage2_records = select_records_with_successful_jobs(records, args.max_records, updated_successful_records)
    
    if args.max_records and len(stage2_records) > args.max_records:
        stage2_records = stage2_records[:args.max_records]
        print(f"ğŸ“Œ é™åˆ¶å¤„ç†å‰{args.max_records}æ¡è®°å½•")
    
    print(f"ğŸ“„ ç»“æœå°†ä¿å­˜åˆ°: {output_file}")
    print(f"ğŸ“Š ç¬¬äºŒé˜¶æ®µæ€»è®¡éœ€è¦å¤„ç† {len(stage2_records)} æ¡è®°å½•")
    
    # ç¨‹åºå¼€å§‹å‰åˆ é™¤å¤‡ä»½æ–‡ä»¶
    if backup_file.exists():
        print(f"ğŸ—‘ï¸ åˆ é™¤æ—§çš„å¤‡ä»½æ–‡ä»¶: {backup_file}")
        backup_file.unlink()
    
    # å¤„ç†ç¬¬äºŒé˜¶æ®µæ‰€æœ‰è®°å½•
    all_records_to_process = stage2_records
    
    if not all_records_to_process:
        print(f"âœ… ç¬¬äºŒé˜¶æ®µæ²¡æœ‰éœ€è¦å¤„ç†çš„è®°å½•ï¼")
        print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"   æ€»è®°å½•æ•°: {len(records)}")
        print(f"   å¾…å¤„ç†è®°å½•æ•°: 0")
    else:
        print(f"ğŸ“‹ ç¬¬äºŒé˜¶æ®µéœ€è¦å¤„ç† {len(all_records_to_process)} æ¡è®°å½•")
        
        # æ‰¹é‡å¤„ç†ç¬¬äºŒé˜¶æ®µ
        success_count = 0
        total_start_time = time.time()
        results = []  # å­˜å‚¨æ‰€æœ‰ç»“æœè®°å½•
        
        for i, record in enumerate(all_records_to_process, 1):
            print(f"\nğŸ“ˆ ç¬¬äºŒé˜¶æ®µè¿›åº¦: {i}/{len(all_records_to_process)} ({i/len(all_records_to_process)*100:.1f}%)")
            
            # æ¯æ¬¡éƒ½è¿è¡Œå¤ç°æµ‹è¯•ï¼Œä¸æ£€æŸ¥æ˜¯å¦å·²æˆåŠŸ
            result_record = run_single_reproduce(record, args.project_name, args.timeout)
            results.append(result_record)
            
            if result_record.get('reproduce', False):
                success_count += 1
            
            # æ¯æ¬¡å¤„ç†å®Œä¸€æ¡è®°å½•åï¼Œç«‹å³å†™å…¥å¤‡ä»½æ–‡ä»¶ï¼ˆçƒ­æ›´æ–°ï¼‰
            print(f"ğŸ“ æ­£åœ¨ä¿å­˜ç¬¬ {i} æ¡è®°å½•åˆ°å¤‡ä»½æ–‡ä»¶...")
            with open(backup_file, 'w', encoding='utf-8') as f:
                for result_record in results:
                    f.write(json.dumps(result_record, ensure_ascii=False) + '\n')
            
            # æ¯å¤„ç†10æ¡è®°å½•æ˜¾ç¤ºä¸€æ¬¡ç»Ÿè®¡
            if i % 10 == 0 or i == len(all_records_to_process):
                elapsed = time.time() - total_start_time
                avg_time = elapsed / i
                remaining = (len(all_records_to_process) - i) * avg_time
                
                print(f"\nğŸ“Š ç¬¬äºŒé˜¶æ®µé˜¶æ®µæ€§ç»Ÿè®¡ ({i}/{len(all_records_to_process)}):")
                print(f"   æˆåŠŸ: {success_count}/{i} ({success_count/i*100:.1f}%)")
                print(f"   å¹³å‡è€—æ—¶: {avg_time:.1f}ç§’/æ¡")
                print(f"   å·²ç”¨æ—¶é—´: {elapsed/60:.1f}åˆ†é’Ÿ")
                print(f"   é¢„è®¡å‰©ä½™: {remaining/60:.1f}åˆ†é’Ÿ")
        
        # å¤„ç†å®Œæˆåï¼Œå°†å¤‡ä»½æ–‡ä»¶æ›¿æ¢ä¸ºæœ€ç»ˆæ–‡ä»¶å¹¶åˆ é™¤å¤‡ä»½æ–‡ä»¶
        print(f"ğŸ“ æ­£åœ¨å°†å¤‡ä»½æ–‡ä»¶æ›¿æ¢ä¸ºæœ€ç»ˆæ–‡ä»¶...")
        import os
        os.replace(backup_file, output_file)
        print(f"âœ… ç¬¬äºŒé˜¶æ®µæ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
        # ç¬¬äºŒé˜¶æ®µæœ€ç»ˆç»Ÿè®¡
        total_elapsed = time.time() - total_start_time
        
        # è®¡ç®—æ€»ä½“æˆåŠŸæ•°é‡ï¼ˆåŒ…æ‹¬ä¹‹å‰æˆåŠŸçš„è®°å½•ï¼‰
        total_success_count = success_count  # ç°åœ¨success_countå·²ç»åŒ…å«äº†æ‰€æœ‰æˆåŠŸçš„è®°å½•
        
        print(f"\nğŸ¯ ç¬¬äºŒé˜¶æ®µæ‰¹é‡å¤„ç†å®Œæˆï¼")
        print(f"=" * 80)
        print(f"ğŸ“Š ç¬¬äºŒé˜¶æ®µæœ€ç»ˆç»Ÿè®¡:")
        print(f"   æ€»è®°å½•æ•°: {len(records)}")
        print(f"   æœ¬æ¬¡å¤„ç†è®°å½•æ•°: {len(all_records_to_process)}")
        print(f"   æœ¬æ¬¡æˆåŠŸæ•°é‡: {success_count}")
        print(f"   æœ¬æ¬¡å¤±è´¥æ•°é‡: {len(all_records_to_process) - success_count}")
        print(f"   æ€»æˆåŠŸæ•°é‡: {total_success_count}")
        if len(all_records_to_process) > 0:
            print(f"   æœ¬æ¬¡æˆåŠŸç‡: {success_count/len(all_records_to_process)*100:.1f}%")
        print(f"   æ€»ä½“æˆåŠŸç‡: {total_success_count/len(records)*100:.1f}%")
        print(f"   æ€»è€—æ—¶: {total_elapsed/60:.1f}åˆ†é’Ÿ")
        if len(all_records_to_process) > 0:
            print(f"   å¹³å‡è€—æ—¶: {total_elapsed/len(all_records_to_process):.1f}ç§’/æ¡")
        print(f"ğŸ“„ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
        # ç”Ÿæˆç¬¬äºŒé˜¶æ®µç®€è¦ç»Ÿè®¡æ–‡ä»¶
        summary_file = output_dir / f"{args.project_name}_batch_reproduce_summary.json"
        summary = {
            'total_records': len(records),
            'processed_records': len(all_records_to_process),
            'success_count': success_count,
            'failure_count': len(all_records_to_process) - success_count,
            'total_success_count': total_success_count,
            'success_rate': success_count / len(all_records_to_process) if len(all_records_to_process) > 0 else 0,
            'overall_success_rate': total_success_count / len(records) if len(records) > 0 else 0,
            'total_elapsed_seconds': total_elapsed,
            'average_time_per_record': total_elapsed / len(all_records_to_process) if len(all_records_to_process) > 0 else 0,
            'timestamp': datetime.now().isoformat(),
            'output_file': str(output_file),
            'stage': 2
        }
        
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“ˆ ç¬¬äºŒé˜¶æ®µç»Ÿè®¡æ‘˜è¦å·²ä¿å­˜åˆ°: {summary_file}")
    
    print(f"\nğŸ‰ æ‰€æœ‰é˜¶æ®µå¤„ç†å®Œæˆï¼")
    print(f"=" * 80)

if __name__ == "__main__":
    main() 